{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "# 1. KONFIGURASI PATH\n",
    "os.environ['HADOOP_HOME'] = \"C:\\\\hadoop\"\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# 2. DEFINISI FLAG JAVA 21 (Manual String)\n",
    "# Kita susun manual agar tidak ada koma/tanda kurung yang salah dibaca Windows\n",
    "java_flags = (\n",
    "    \"--add-opens=java.base/java.lang=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.lang.invoke=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.lang.reflect=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.io=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.net=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.nio=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.util=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.util.concurrent=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/sun.nio.cs=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/sun.security.action=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/sun.util.calendar=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/sun.misc=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/jdk.internal.misc=ALL-UNNAMED \"\n",
    "    \"-Dio.netty.tryReflectionSetAccessible=true\"\n",
    ")\n",
    "\n",
    "# 3. BAJAK PELUNCUR SPARK (THE NUCLEAR OPTION)\n",
    "# Kita paksa Spark menggunakan arguments ini saat start. \n",
    "# Kita gabungkan: Driver Options + Packages Delta + Mode Pyspark Shell\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--driver-java-options \"{java_flags}\" --packages io.delta:delta-spark_2.12:3.2.0 pyspark-shell'\n",
    "\n",
    "# Import Spark SETELAH set environment variable di atas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, monotonically_increasing_id\n",
    "from delta import *\n",
    "\n",
    "# 4. BUILDER MINIMALIS\n",
    "# Kita tidak perlu config aneh-aneh lagi karena sudah kita inject di atas\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"ProyekSparkWindows_Nuclear\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Start\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Bungkam Log\n",
    "warnings.filterwarnings('ignore')\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f9b4e65-714c-46a7-b0ca-2c22d87e349e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Chapter 6: SQL Queries in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b6f89b-2d68-4937-a30a-ac64ef7caa49",
     "showTitle": true,
     "title": "Create Salary dataframe"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+\n",
      "| ID|Employee|Department|Salary|Age|\n",
      "+---+--------+----------+------+---+\n",
      "|  1|    John| Field-eng|  3500| 40|\n",
      "|  2|  Robert|     Sales|  4000| 38|\n",
      "|  3|   Maria|   Finance|  3500| 28|\n",
      "|  4| Michael|     Sales|  3000| 20|\n",
      "|  5|   Kelly|   Finance|  3500| 35|\n",
      "|  6|    Kate|   Finance|  3000| 45|\n",
      "|  7|  Martin|   Finance|  3500| 26|\n",
      "|  8|   Kiran|     Sales|  2200| 35|\n",
      "+---+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Membuat DataFrame Secara Manual (Creating DataFrame manually)\n",
    "\n",
    "salary_data_with_id = [(1, \"John\", \"Field-eng\", 3500, 40), \\\n",
    "    (2, \"Robert\", \"Sales\", 4000, 38), \\\n",
    "    (3, \"Maria\", \"Finance\", 3500, 28), \\\n",
    "    (4, \"Michael\", \"Sales\", 3000, 20), \\\n",
    "    (5, \"Kelly\", \"Finance\", 3500, 35), \\\n",
    "    (6, \"Kate\", \"Finance\", 3000, 45), \\\n",
    "    (7, \"Martin\", \"Finance\", 3500, 26), \\\n",
    "    (8, \"Kiran\", \"Sales\", 2200, 35), \\\n",
    "  ]\n",
    "columns= [\"ID\", \"Employee\", \"Department\", \"Salary\", \"Age\"]\n",
    "salary_data_with_id = spark.createDataFrame(data = salary_data_with_id, schema = columns)\n",
    "salary_data_with_id.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e075be3c-bb49-4c81-a7b9-7dfbb4370056",
     "showTitle": true,
     "title": "Writing csv file"
    }
   },
   "outputs": [],
   "source": [
    "# Konteks Penyimpanan CSV (Saving and Reading CSV Context)\n",
    "\n",
    "salary_data_with_id.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"salary_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a11fe3af-e723-4e97-abbf-d503acf033e3",
     "showTitle": true,
     "title": "Reading csv file"
    }
   },
   "outputs": [],
   "source": [
    "csv_data = spark.read.csv('salary_data.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1aa8d82-a393-448a-8fd0-5110b1eb1af2",
     "showTitle": true,
     "title": "Showing data"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+\n",
      "| ID|Employee|Department|Salary|Age|\n",
      "+---+--------+----------+------+---+\n",
      "|  1|    John| Field-eng|  3500| 40|\n",
      "|  7|  Martin|   Finance|  3500| 26|\n",
      "|  3|   Maria|   Finance|  3500| 28|\n",
      "|  4| Michael|     Sales|  3000| 20|\n",
      "|  5|   Kelly|   Finance|  3500| 35|\n",
      "|  2|  Robert|     Sales|  4000| 38|\n",
      "|  6|    Kate|   Finance|  3000| 45|\n",
      "|  8|   Kiran|     Sales|  2200| 35|\n",
      "+---+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f186780-97f0-4b7f-af17-c33073c872ce",
     "showTitle": true,
     "title": "# Perform transformations on the loaded data"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+\n",
      "| ID|Employee|Department|Salary|Age|\n",
      "+---+--------+----------+------+---+\n",
      "|  1|    John| Field-eng|  3500| 40|\n",
      "|  7|  Martin|   Finance|  3500| 26|\n",
      "|  3|   Maria|   Finance|  3500| 28|\n",
      "|  5|   Kelly|   Finance|  3500| 35|\n",
      "|  2|  Robert|     Sales|  4000| 38|\n",
      "+---+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Melakukan Transformasi pada Data (Perform transformations on the loaded data)\n",
    "\n",
    "# Perform transformations on the loaded data \n",
    "processed_data = csv_data.filter(csv_data[\"Salary\"] > 3000) \n",
    "# Save the processed data as a table \n",
    "processed_data.createOrReplaceTempView(\"high_salary_employees\") \n",
    "# Perform SQL queries on the saved table \n",
    "results = spark.sql(\"SELECT * FROM high_salary_employees \") \n",
    "results.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "022c96b3-91f2-41ea-91c4-58ecd511a2f6",
     "showTitle": true,
     "title": "Saving Transformed Data as a View"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+---+\n",
      "|Employee|Department|Salary|Age|\n",
      "+--------+----------+------+---+\n",
      "|    John| Field-eng|  3500| 40|\n",
      "|  Robert|     Sales|  4000| 38|\n",
      "|   Kelly|   Finance|  3500| 35|\n",
      "|    Kate|   Finance|  3000| 45|\n",
      "|   Kiran|     Sales|  2200| 35|\n",
      "+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Memanfaatkan Spark SQL untuk memfilter dan memilih data (Utilizing Spark SQL to filter and select data based on specific criteria)\n",
    "\n",
    "# Save the processed data as a view \n",
    "salary_data_with_id.createOrReplaceTempView(\"employees\") \n",
    "#Apply filtering on data\n",
    "filtered_data = spark.sql(\"SELECT Employee, Department, Salary, Age FROM employees WHERE age > 30\") \n",
    "# Display the results \n",
    "filtered_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174c84ac-fa9e-42a7-a3b5-a166193e63b0",
     "showTitle": true,
     "title": "Aggregating data"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|average_salary|\n",
      "+--------------+\n",
      "|        3275.0|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agregasi (Aggregation)\n",
    "\n",
    "# Perform an aggregation to calculate the average salary \n",
    "average_salary = spark.sql(\"SELECT AVG(Salary) AS average_salary FROM employees\") \n",
    "# Display the average salary \n",
    "average_salary.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97bfd3f0-1a65-4a58-bdc7-58b55dd58840",
     "showTitle": true,
     "title": "Sorting data"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+\n",
      "| ID|Employee|Department|Salary|Age|\n",
      "+---+--------+----------+------+---+\n",
      "|  2|  Robert|     Sales|  4000| 38|\n",
      "|  3|   Maria|   Finance|  3500| 28|\n",
      "|  1|    John| Field-eng|  3500| 40|\n",
      "|  5|   Kelly|   Finance|  3500| 35|\n",
      "|  7|  Martin|   Finance|  3500| 26|\n",
      "|  4| Michael|     Sales|  3000| 20|\n",
      "|  6|    Kate|   Finance|  3000| 45|\n",
      "|  8|   Kiran|     Sales|  2200| 35|\n",
      "+---+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pengurutan (Sorting)\n",
    "\n",
    "# Sort the data based on the salary column in descending order \n",
    "sorted_data = spark.sql(\"SELECT * FROM employees ORDER BY Salary DESC\") \n",
    "# Display the sorted data \n",
    "sorted_data.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38606a23-aa13-49ca-b7dc-d669dd472f55",
     "showTitle": true,
     "title": "Combining Aggregations"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+---+\n",
      "|Employee|Department|Salary|Age|\n",
      "+--------+----------+------+---+\n",
      "|  Robert|     Sales|  4000| 38|\n",
      "|   Kelly|   Finance|  3500| 35|\n",
      "|    John| Field-eng|  3500| 40|\n",
      "+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Menggabungkan agregasi (Combining aggregations)\n",
    "\n",
    "# Sort the data based on the salary column in descending order \n",
    "filtered_data = spark.sql(\"SELECT Employee, Department, Salary, Age FROM employees WHERE age > 30 AND Salary > 3000 ORDER BY Salary DESC\") \n",
    "# Display the results \n",
    "filtered_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7701c892-0883-4bc2-9b5e-f51cf55fcc78",
     "showTitle": true,
     "title": "Grouping data"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|Department|       avg(Salary)|\n",
      "+----------+------------------+\n",
      "| Field-eng|            3500.0|\n",
      "|     Sales|3066.6666666666665|\n",
      "|   Finance|            3375.0|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mengelompokkan data (Grouping data)\n",
    "\n",
    "# Group the data based on the Department column and take average salary for each department  \n",
    "grouped_data = spark.sql(\"SELECT Department, avg(Salary) FROM employees GROUP BY Department\") \n",
    "# Display the results \n",
    "grouped_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abafb986-88fa-4c43-8f65-6f7bbfa70ec6",
     "showTitle": true,
     "title": "Grouping with multiple aggregations"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+\n",
      "|Department|total_salary|max_salary|\n",
      "+----------+------------+----------+\n",
      "| Field-eng|        3500|      3500|\n",
      "|     Sales|        9200|      4000|\n",
      "|   Finance|       13500|      3500|\n",
      "+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mengagregasi data (Aggregating data)\n",
    "\n",
    "# Perform grouping and multiple aggregations  \n",
    "aggregated_data = spark.sql(\"SELECT Department, sum(Salary) AS total_salary, max(Salary) AS max_salary FROM employees GROUP BY Department\") \n",
    "\n",
    "# Display the results \n",
    "aggregated_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b85baa3-2e70-4038-af6c-440346d96d78",
     "showTitle": true,
     "title": "Window functions"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+--------------+\n",
      "| ID|Employee|Department|Salary|Age|cumulative_sum|\n",
      "+---+--------+----------+------+---+--------------+\n",
      "|  1|    John| Field-eng|  3500| 40|          3500|\n",
      "|  7|  Martin|   Finance|  3500| 26|          3500|\n",
      "|  3|   Maria|   Finance|  3500| 28|          7000|\n",
      "|  5|   Kelly|   Finance|  3500| 35|         10500|\n",
      "|  6|    Kate|   Finance|  3000| 45|         13500|\n",
      "|  4| Michael|     Sales|  3000| 20|          3000|\n",
      "|  8|   Kiran|     Sales|  2200| 35|          5200|\n",
      "|  2|  Robert|     Sales|  4000| 38|          9200|\n",
      "+---+--------+----------+------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Menghitung jumlah kumulatif menggunakan fungsi jendela (Calculating cumulative sum using window functions)\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"Department\").orderBy(\"Age\")\n",
    "\n",
    "# Calculate the cumulative sum using window function\n",
    "df_with_cumulative_sum = salary_data_with_id.withColumn(\"cumulative_sum\", sum(col(\"Salary\")).over(window_spec))\n",
    "\n",
    "# Display the result\n",
    "df_with_cumulative_sum.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8c6978-6e33-47db-8ad6-8af7cd77d522",
     "showTitle": true,
     "title": "Using udfs"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+----------------+\n",
      "| ID|Employee|Department|Salary|Age|capitalized_name|\n",
      "+---+--------+----------+------+---+----------------+\n",
      "|  1|    John| Field-eng|  3500| 40|            JOHN|\n",
      "|  2|  Robert|     Sales|  4000| 38|          ROBERT|\n",
      "|  3|   Maria|   Finance|  3500| 28|           MARIA|\n",
      "|  4| Michael|     Sales|  3000| 20|         MICHAEL|\n",
      "|  5|   Kelly|   Finance|  3500| 35|           KELLY|\n",
      "|  6|    Kate|   Finance|  3000| 45|            KATE|\n",
      "|  7|  Martin|   Finance|  3500| 26|          MARTIN|\n",
      "|  8|   Kiran|     Sales|  2200| 35|           KIRAN|\n",
      "+---+--------+----------+------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Menerapkan UDF ke DataFrame (Applying a UDF to a DataFrame)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a UDF to capitalize a string\n",
    "capitalize_udf = udf(lambda x: x.upper(), StringType())\n",
    "\n",
    "# Apply the UDF to a column\n",
    "df_with_capitalized_names = salary_data_with_id.withColumn(\"capitalized_name\", capitalize_udf(\"Employee\"))\n",
    "\n",
    "# Display the result\n",
    "df_with_capitalized_names.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1d9134-f8bf-4597-9b46-1e0142a0acac",
     "showTitle": true,
     "title": "Applying functions"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|pandas_plus_one(Salary)|\n",
      "+-----------------------+\n",
      "|                   3501|\n",
      "|                   4001|\n",
      "|                   3501|\n",
      "|                   3001|\n",
      "|                   3501|\n",
      "|                   3001|\n",
      "|                   3501|\n",
      "|                   2201|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# @pandas_udf('long')\n",
    "# def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "#     # Simply plus one by using pandas Series.\n",
    "#     return series + 1\n",
    "\n",
    "# salary_data_with_id.select(pandas_plus_one(salary_data_with_id.Salary)).show()\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "@udf(returnType=LongType())\n",
    "def plus_one(value):\n",
    "    if value is not None:\n",
    "        return value + 1\n",
    "    return None\n",
    "\n",
    "salary_data_with_id.select(plus_one(salary_data_with_id.Salary).alias(\"pandas_plus_one(Salary)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0141f5b-f209-4f66-928f-1d631a99ca58",
     "showTitle": true,
     "title": "Pandas udfs"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|add_one(Salary)|\n",
      "+---------------+\n",
      "|           3501|\n",
      "|           4001|\n",
      "|           3501|\n",
      "|           3001|\n",
      "|           3501|\n",
      "|           3001|\n",
      "|           3501|\n",
      "|           2201|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @pandas_udf(\"integer\")\n",
    "# def add_one(s: pd.Series) -> pd.Series:\n",
    "#     return s + 1\n",
    "\n",
    "# spark.udf.register(\"add_one\", add_one)\n",
    "# spark.sql(\"SELECT add_one(Salary) FROM employees\").show()\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "salary_data_with_id.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "def add_one_logic(value):\n",
    "    if value is not None:\n",
    "        return value + 1\n",
    "    return None\n",
    "\n",
    "spark.udf.register(\"add_one\", add_one_logic, IntegerType())\n",
    "\n",
    "spark.sql(\"SELECT add_one(Salary) FROM employees\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 969987236417588,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Chapter 6 Code",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
