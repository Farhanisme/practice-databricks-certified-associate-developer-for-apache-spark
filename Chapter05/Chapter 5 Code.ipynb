{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import * \n",
    "\n",
    "os.environ['HADOOP_HOME'] = \"C:\\\\hadoop\"\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "jvm_options = (\n",
    "    \"--add-opens=java.base/java.lang=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.lang.invoke=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.lang.reflect=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.io=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.net=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.nio=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.util=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.util.concurrent=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/sun.nio.cs=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/sun.security.action=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/sun.util.calendar=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\"\n",
    ")\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"ProyekSparkWindows_Delta\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", jvm_options) \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", jvm_options) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0\") \n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f1436a0-3357-4850-b507-a12c76e60c22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Chapter 5: Advanced Operations in Spark Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c029f8c-dfbc-4e10-b09d-ccbea7b62eec",
     "showTitle": true,
     "title": "Create Salary dataframe"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n",
      "+--------+----------+------+\n",
      "|Employee|Department|Salary|\n",
      "+--------+----------+------+\n",
      "|    John| Field-eng|  3500|\n",
      "| Michael| Field-eng|  4500|\n",
      "|  Robert|      NULL|  4000|\n",
      "|   Maria|   Finance|  3500|\n",
      "|    John|     Sales|  3000|\n",
      "|   Kelly|   Finance|  3500|\n",
      "|    Kate|   Finance|  3000|\n",
      "|  Martin|      NULL|  3500|\n",
      "|   Kiran|     Sales|  2200|\n",
      "| Michael| Field-eng|  4500|\n",
      "+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_data = [(\"John\", \"Field-eng\", 3500), \n",
    "    (\"Michael\", \"Field-eng\", 4500), \n",
    "    (\"Robert\", None, 4000), \n",
    "    (\"Maria\", \"Finance\", 3500), \n",
    "    (\"John\", \"Sales\", 3000), \n",
    "    (\"Kelly\", \"Finance\", 3500), \n",
    "    (\"Kate\", \"Finance\", 3000), \n",
    "    (\"Martin\", None, 3500), \n",
    "    (\"Kiran\", \"Sales\", 2200), \n",
    "    (\"Michael\", \"Field-eng\", 4500) \n",
    "  ]\n",
    "columns= [\"Employee\", \"Department\", \"Salary\"]\n",
    "salary_data = spark.createDataFrame(data = salary_data, schema = columns)\n",
    "salary_data.printSchema()\n",
    "salary_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c64523b-97f8-4cdf-8c73-13723a7f7453",
     "showTitle": true,
     "title": "Using Groupby in a Dataframe"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupedData[grouping expressions: [Department], value: [Employee: string, Department: string ... 1 more field], type: GroupBy]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menggunakan groupBy dalam DataFrame (Using groupBy in a DataFrame)\n",
    "\n",
    "salary_data.groupby('Department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e2c600-8160-4138-968f-835e6757f06c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|Department|       avg(Salary)|\n",
      "+----------+------------------+\n",
      "| Field-eng| 4166.666666666667|\n",
      "|      NULL|            3750.0|\n",
      "|   Finance|3333.3333333333335|\n",
      "|     Sales|            2600.0|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_data.groupby('Department').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d437c9f0-2336-4687-83b4-7c8142b4085f",
     "showTitle": true,
     "title": "Complex Groupby Statement"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|Department|Salary|\n",
      "+----------+------+\n",
      "|      NULL|  7500|\n",
      "| Field-eng| 12500|\n",
      "|   Finance| 10000|\n",
      "|     Sales|  5200|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pernyataan groupBy yang kompleks (A complex groupBy statement)\n",
    "\n",
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "salary_data.groupBy('Department')\\\n",
    "  .sum('Salary')\\\n",
    "  .withColumn('sum(Salary)',round(col('sum(Salary)'), 2))\\\n",
    "  .withColumnRenamed('sum(Salary)', 'Salary')\\\n",
    "  .orderBy('Department')\\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc73dea-aa0c-4a54-aded-a4c3814f01a9",
     "showTitle": true,
     "title": "Joining Dataframes in Spark"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+\n",
      "| ID|Employee|Department|Salary|\n",
      "+---+--------+----------+------+\n",
      "|  1|    John| Field-eng|  3500|\n",
      "|  2|  Robert|     Sales|  4000|\n",
      "|  3|   Maria|   Finance|  3500|\n",
      "|  4| Michael|     Sales|  3000|\n",
      "|  5|   Kelly|   Finance|  3500|\n",
      "|  6|    Kate|   Finance|  3000|\n",
      "|  7|  Martin|   Finance|  3500|\n",
      "|  8|   Kiran|     Sales|  2200|\n",
      "+---+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Menggabungkan DataFrames di Spark (Joining DataFrames in Spark)\n",
    "\n",
    "salary_data_with_id = [(1, \"John\", \"Field-eng\", 3500), \\\n",
    "    (2, \"Robert\", \"Sales\", 4000), \\\n",
    "    (3, \"Maria\", \"Finance\", 3500), \\\n",
    "    (4, \"Michael\", \"Sales\", 3000), \\\n",
    "    (5, \"Kelly\", \"Finance\", 3500), \\\n",
    "    (6, \"Kate\", \"Finance\", 3000), \\\n",
    "    (7, \"Martin\", \"Finance\", 3500), \\\n",
    "    (8, \"Kiran\", \"Sales\", 2200), \\\n",
    "  ]\n",
    "columns= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\n",
    "salary_data_with_id = spark.createDataFrame(data = salary_data_with_id, schema = columns)\n",
    "salary_data_with_id.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "125e73d8-c716-4e1c-8900-859c1ec666e9",
     "showTitle": true,
     "title": "Employee data"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| ID|State|Gender|\n",
      "+---+-----+------+\n",
      "|  1|   NY|     M|\n",
      "|  2|   NC|     M|\n",
      "|  3|   NY|     F|\n",
      "|  4|   TX|     M|\n",
      "|  5|   NY|     F|\n",
      "|  6|   AZ|     F|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_data = [(1, \"NY\", \"M\"), \\\n",
    "    (2, \"NC\", \"M\"), \\\n",
    "    (3, \"NY\", \"F\"), \\\n",
    "    (4, \"TX\", \"M\"), \\\n",
    "    (5, \"NY\", \"F\"), \\\n",
    "    (6, \"AZ\", \"F\") \\\n",
    "  ]\n",
    "columns= [\"ID\", \"State\", \"Gender\"]\n",
    "employee_data = spark.createDataFrame(data = employee_data, schema = columns)\n",
    "employee_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0137bf4-d318-4417-86ca-df79f2fb80be",
     "showTitle": true,
     "title": "Inner join"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+-----+------+\n",
      "| ID|Employee|Department|Salary| ID|State|Gender|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "|  1|    John| Field-eng|  3500|  1|   NY|     M|\n",
      "|  2|  Robert|     Sales|  4000|  2|   NC|     M|\n",
      "|  3|   Maria|   Finance|  3500|  3|   NY|     F|\n",
      "|  4| Michael|     Sales|  3000|  4|   TX|     M|\n",
      "|  5|   Kelly|   Finance|  3500|  5|   NY|     F|\n",
      "|  6|    Kate|   Finance|  3000|  6|   AZ|     F|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inner joins (Inner joins)\n",
    "\n",
    "salary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34ff657-b0dd-4485-96f0-6d7c6126a1bd",
     "showTitle": true,
     "title": "Outer join"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+----+-----+------+\n",
      "| ID|Employee|Department|Salary|  ID|State|Gender|\n",
      "+---+--------+----------+------+----+-----+------+\n",
      "|  1|    John| Field-eng|  3500|   1|   NY|     M|\n",
      "|  2|  Robert|     Sales|  4000|   2|   NC|     M|\n",
      "|  3|   Maria|   Finance|  3500|   3|   NY|     F|\n",
      "|  4| Michael|     Sales|  3000|   4|   TX|     M|\n",
      "|  5|   Kelly|   Finance|  3500|   5|   NY|     F|\n",
      "|  6|    Kate|   Finance|  3000|   6|   AZ|     F|\n",
      "|  7|  Martin|   Finance|  3500|NULL| NULL|  NULL|\n",
      "|  8|   Kiran|     Sales|  2200|NULL| NULL|  NULL|\n",
      "+---+--------+----------+------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Outer joins (Outer joins)\n",
    "\n",
    "salary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,\"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "868ca315-ab44-4eb6-b8f1-92481d770911",
     "showTitle": true,
     "title": "Left join"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+----+-----+------+\n",
      "| ID|Employee|Department|Salary|  ID|State|Gender|\n",
      "+---+--------+----------+------+----+-----+------+\n",
      "|  1|    John| Field-eng|  3500|   1|   NY|     M|\n",
      "|  2|  Robert|     Sales|  4000|   2|   NC|     M|\n",
      "|  3|   Maria|   Finance|  3500|   3|   NY|     F|\n",
      "|  4| Michael|     Sales|  3000|   4|   TX|     M|\n",
      "|  5|   Kelly|   Finance|  3500|   5|   NY|     F|\n",
      "|  6|    Kate|   Finance|  3000|   6|   AZ|     F|\n",
      "|  7|  Martin|   Finance|  3500|NULL| NULL|  NULL|\n",
      "|  8|   Kiran|     Sales|  2200|NULL| NULL|  NULL|\n",
      "+---+--------+----------+------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Left joins (Left joins)\n",
    "\n",
    "salary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cba2965-54b3-4d04-a456-77e9d9af6e1f",
     "showTitle": true,
     "title": "Right join"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+-----+------+\n",
      "| ID|Employee|Department|Salary| ID|State|Gender|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "|  1|    John| Field-eng|  3500|  1|   NY|     M|\n",
      "|  2|  Robert|     Sales|  4000|  2|   NC|     M|\n",
      "|  3|   Maria|   Finance|  3500|  3|   NY|     F|\n",
      "|  4| Michael|     Sales|  3000|  4|   TX|     M|\n",
      "|  5|   Kelly|   Finance|  3500|  5|   NY|     F|\n",
      "|  6|    Kate|   Finance|  3000|  6|   AZ|     F|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Right joins (Right joins)\n",
    "\n",
    "salary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,\"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9f95c1-4109-4ceb-925d-7b10cf838fdd",
     "showTitle": true,
     "title": "Union"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Employee: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n",
      "+---+--------+----------+------+\n",
      "|ID |Employee|Department|Salary|\n",
      "+---+--------+----------+------+\n",
      "|1  |John    |Field-eng |3500  |\n",
      "|2  |Robert  |Sales     |4000  |\n",
      "|3  |Aliya   |Finance   |3500  |\n",
      "|4  |Nate    |Sales     |3000  |\n",
      "+---+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross joins (Cross joins)\n",
    "\n",
    "salary_data_with_id_2 = [(1, \"John\", \"Field-eng\", 3500), \\\n",
    "    (2, \"Robert\", \"Sales\", 4000), \\\n",
    "    (3, \"Aliya\", \"Finance\", 3500), \\\n",
    "    (4, \"Nate\", \"Sales\", 3000), \\\n",
    "  ]\n",
    "columns2= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\n",
    "\n",
    "salary_data_with_id_2 = spark.createDataFrame(data = salary_data_with_id_2, schema = columns2)\n",
    "\n",
    "salary_data_with_id_2.printSchema()\n",
    "salary_data_with_id_2.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb3d433-2a89-47b4-9d21-2d79194809c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+\n",
      "|ID |Employee|Department|Salary|\n",
      "+---+--------+----------+------+\n",
      "|1  |John    |Field-eng |3500  |\n",
      "|2  |Robert  |Sales     |4000  |\n",
      "|3  |Maria   |Finance   |3500  |\n",
      "|4  |Michael |Sales     |3000  |\n",
      "|5  |Kelly   |Finance   |3500  |\n",
      "|6  |Kate    |Finance   |3000  |\n",
      "|7  |Martin  |Finance   |3500  |\n",
      "|8  |Kiran   |Sales     |2200  |\n",
      "|1  |John    |Field-eng |3500  |\n",
      "|2  |Robert  |Sales     |4000  |\n",
      "|3  |Aliya   |Finance   |3500  |\n",
      "|4  |Nate    |Sales     |3000  |\n",
      "+---+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionDF = salary_data_with_id.union(salary_data_with_id_2)\n",
    "unionDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d84b0031-6f62-41a8-9533-b510a487ab0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading and Writing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c8eb85-7d75-4010-977d-370b7940b57e",
     "showTitle": true,
     "title": "Reading and writing CSV files"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+\n",
      "| ID|Employee|Department|Salary|\n",
      "+---+--------+----------+------+\n",
      "|  1|    John| Field-eng|  3500|\n",
      "|  7|  Martin|   Finance|  3500|\n",
      "|  3|   Maria|   Finance|  3500|\n",
      "|  4| Michael|     Sales|  3000|\n",
      "|  5|   Kelly|   Finance|  3500|\n",
      "|  2|  Robert|     Sales|  4000|\n",
      "|  6|    Kate|   Finance|  3000|\n",
      "|  8|   Kiran|     Sales|  2200|\n",
      "+---+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Membaca dan menulis file CSV (Reading and writing CSV files)\n",
    "\n",
    "salary_data_with_id.write.csv('salary_data.csv', mode='overwrite', header=True)\n",
    "spark.read.csv('salary_data.csv', header=True).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b033bc47-7a90-4ae1-b37b-692860e06482",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+\n",
      "| ID|  State|   Gender|\n",
      "+---+-------+---------+\n",
      "|  1|   John|Field-eng|\n",
      "|  7| Martin|  Finance|\n",
      "|  3|  Maria|  Finance|\n",
      "|  4|Michael|    Sales|\n",
      "|  5|  Kelly|  Finance|\n",
      "|  2| Robert|    Sales|\n",
      "|  6|   Kate|  Finance|\n",
      "|  8|  Kiran|    Sales|\n",
      "+---+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "filePath = 'salary_data.csv'\n",
    "columns= [\"ID\", \"State\", \"Gender\"] \n",
    "schema = StructType([\n",
    "      StructField(\"ID\", IntegerType(),True),\n",
    "  StructField(\"State\",  StringType(),True),\n",
    "  StructField(\"Gender\",  StringType(),True)\n",
    "])\n",
    " \n",
    "read_data = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schema).load(filePath)\n",
    "read_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd8f639-d141-48c9-be8e-dffd764aa0ee",
     "showTitle": true,
     "title": "Reading and writing Parquet files"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+\n",
      "| ID|Employee|Department|Salary|\n",
      "+---+--------+----------+------+\n",
      "|  1|    John| Field-eng|  3500|\n",
      "|  7|  Martin|   Finance|  3500|\n",
      "|  3|   Maria|   Finance|  3500|\n",
      "|  4| Michael|     Sales|  3000|\n",
      "|  5|   Kelly|   Finance|  3500|\n",
      "|  2|  Robert|     Sales|  4000|\n",
      "|  6|    Kate|   Finance|  3000|\n",
      "|  8|   Kiran|     Sales|  2200|\n",
      "+---+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Membaca dan menulis file Parquet (Reading and writing Parquet files)\n",
    "\n",
    "salary_data_with_id.write.parquet('salary_data.parquet', mode='overwrite')\n",
    "spark.read.parquet('salary_data.parquet').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492b344b-3719-44cd-a8dc-034d20f3a409",
     "showTitle": true,
     "title": "Reading and writing ORC files"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+\n",
      "| ID|Employee|Department|Salary|\n",
      "+---+--------+----------+------+\n",
      "|  3|   Maria|   Finance|  3500|\n",
      "|  4| Michael|     Sales|  3000|\n",
      "|  1|    John| Field-eng|  3500|\n",
      "|  7|  Martin|   Finance|  3500|\n",
      "|  5|   Kelly|   Finance|  3500|\n",
      "|  2|  Robert|     Sales|  4000|\n",
      "|  6|    Kate|   Finance|  3000|\n",
      "|  8|   Kiran|     Sales|  2200|\n",
      "+---+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Membaca dan menulis file ORC (Reading and writing ORC files)\n",
    "\n",
    "salary_data_with_id.write.orc('salary_data.orc', mode='overwrite')\n",
    "spark.read.orc('salary_data.orc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b3c1309-4a00-4a92-ac3e-7f2a9d491445",
     "showTitle": true,
     "title": "Reading and writing Delta files"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+\n",
      "| ID|Employee|Department|Salary|\n",
      "+---+--------+----------+------+\n",
      "|  1|    John| Field-eng|  3500|\n",
      "|  7|  Martin|   Finance|  3500|\n",
      "|  3|   Maria|   Finance|  3500|\n",
      "|  4| Michael|     Sales|  3000|\n",
      "|  5|   Kelly|   Finance|  3500|\n",
      "|  2|  Robert|     Sales|  4000|\n",
      "|  6|    Kate|   Finance|  3000|\n",
      "|  8|   Kiran|     Sales|  2200|\n",
      "+---+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Membaca dan menulis file Delta (Reading and writing Delta files)\n",
    "\n",
    "salary_data_with_id.write.format(\"delta\").save(\"/FileStore/tables/salary_data_with_id\", mode='overwrite')\n",
    "df = spark.read.load(\"/FileStore/tables/salary_data_with_id\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d616d17f-7848-4527-aae3-78eec9d3214d",
     "showTitle": true,
     "title": "Using SQL in Spark"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       8|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Menggunakan SQL di Spark (Using SQL in Spark)\n",
    "\n",
    "salary_data_with_id.createOrReplaceTempView(\"SalaryTable\")\n",
    "spark.sql(\"SELECT count(*) from SalaryTable\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f549a552-a92a-477c-bcbd-0eaf6104c207",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66004b0-07ac-4c06-966e-1370a2e1b3d6",
     "showTitle": true,
     "title": "Catalyst Optimizer in Action"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [employee#1439, department#1440]\n",
      "+- *(1) Filter (isnotnull(salary#1441) AND (salary#1441 > 3500))\n",
      "   +- FileScan csv [Employee#1439,Department#1440,Salary#1441] Batched: false, DataFilters: [isnotnull(Salary#1441), (Salary#1441 > 3500)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/.vscode/denbids/task/final/Databricks-Certified-Associate-Dev..., PartitionFilters: [], PushedFilters: [IsNotNull(Salary), GreaterThan(Salary,3500)], ReadSchema: struct<Employee:string,Department:string,Salary:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SparkSession setup \n",
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName(\"CatalystOptimizerExample\").getOrCreate() \n",
    "# Load data \n",
    "df = spark.read.csv(\"salary_data.csv\", header=True, inferSchema=True) \n",
    "# Query with Catalyst Optimizer \n",
    "result_df = df.select(\"employee\", \"department\").filter(df[\"salary\"] > 3500) \n",
    "# Explain the optimized query plan \n",
    "result_df.explain() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ba28ee-80d0-4210-acb7-4a45bee2815b",
     "showTitle": true,
     "title": "Unpersisting Data"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, Employee: string, Department: string, Salary: int]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persisting dan caching di Apache Spark (Persisting and caching in Apache Spark)\n",
    "\n",
    "# Cache a DataFrame \n",
    "df.cache() \n",
    "# Unpersist the cached DataFrame \n",
    "df.unpersist() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25bff695-c206-4800-80c0-e6dde6962438",
     "showTitle": true,
     "title": "Repartitioning Data"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, Employee: string, Department: string, Salary: int]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition a DataFrame into 8 partitions \n",
    "df.repartition(8) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eb87f1e-ba2d-47fb-a7c1-b74e1f293598",
     "showTitle": true,
     "title": "Coalescing Data"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, Employee: string, Department: string, Salary: int]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coalesce a DataFrame to 4 partitions \n",
    "df.coalesce(4) \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 969987236417588,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Chapter 5 Code",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
